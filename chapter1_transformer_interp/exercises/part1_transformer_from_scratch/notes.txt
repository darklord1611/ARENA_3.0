# Parameters -> weights and biases, learned during training
# Activations -> temporary numbers stored during a forward pass of the model, must be related to a concrete input when we discuss


# advance indexing in Pytorch, access a lookup table of Tensors (shape: (vocab_size, d_model)) using table[(batch_size, seq_len)] -> will return a tensor of shape: (batch_size, seq_len, d_model)


# Calculate cross-entropy loss
- At each position t, the logits are the modelâ€™s predictions for the token at t+1


# Strategies to generate text outputs
- Greedy Search -> select token with highest probs, Beam Search -> at each timestep, keep top k highest prob tokens and then choose the highest one
- Both the above approaches are deterministic and still suffer from repetitive tokens -> introduce penalties like n-gram to keep the output diverse -> introduce stochasticity(random)
-> SAMPLING

- drawn tokens based on conditional probabilities, -> apply low temperature to softmax will make the distribution less random, when temperature -> 0, we go back to greedy decoding
- Top-K sampling, choose the top K most likely next tokens -> renormalize the distribution across those K words then sample from there

- Limitations of top-K, since we always draw K words at a timestep, the creativity is also damaged, model may produce gibberish or seemingly coherent but nonsense content
-> Top-p(Nucleus) Sampling

- Top-P -> define a desired cumulative probability like 0.92 -> find the smallest set of words that have cumulative probablity close to 0.92 -> then normalize the probablity across those words
-> Choose from a dynamic set of words every time
